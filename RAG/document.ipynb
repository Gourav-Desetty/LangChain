{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81567276",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "\n",
    "LangChain document \n",
    "* page content(str)\n",
    "* metadata (dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b34012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71992c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\n",
    "    page_content=\"this is the main content im using in rag paper\",\n",
    "    metadata ={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\": \"Robert Frost\",\n",
    "        \"data-created\": \"2025-09-16\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08bccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afc4543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = {\n",
    "\"../data/text_files/content.txt\":\"\"\" Socompa is a large stratovolcano (composite volcano) on the border of Argentina and Chile.\n",
    "    It has an elevation of 6,051 metres (19,852 ft) and is part of the Chilean and Argentine Andean Volcanic Belt (AVB). \n",
    "    Socompa is within the Central Volcanic Zone, one of the segments of the AVB, which contains about 44 active volcanoes. \n",
    "    It begins in Peru and runs first through Bolivia and Chile, and then Argentina and Chile. \n",
    "    Socompa lies close to the pass of the same name where the Salta-Antofagasta railway crosses the Chilean border.\n",
    "\n",
    "    Most of the northwestern slope of Socompa collapsed catastrophically 7,200 years ago to form an extensive debris avalanche deposit. \n",
    "    The Socompa collapse is among the largest known on land with a volume of 19.2 cubic kilometres (4.6 cu mi) and a surface area of 490 square \n",
    "    kilometres (190 sq mi); its features are well-preserved by the arid climate. The deposit was at first considered to be either a moraine or a \n",
    "    pyroclastic flow deposit, until the 1980 eruption of Mount St. Helens prompted awareness of the instability of volcanic edifices and the \n",
    "    existence of large-scale collapses. There are large toreva blocks, which were left behind within the collapse crater. \n",
    "    After the landslide, the volcano was rebuilt by the effusion of lava flows and much of the scar is now filled in.\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c834b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, content in sample_text.items():\n",
    "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883d164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/content.txt'}, page_content=' Socompa is a large stratovolcano (composite volcano) on the border of Argentina and Chile.\\n    It has an elevation of 6,051 metres (19,852 ft) and is part of the Chilean and Argentine Andean Volcanic Belt (AVB). \\n    Socompa is within the Central Volcanic Zone, one of the segments of the AVB, which contains about 44 active volcanoes. \\n    It begins in Peru and runs first through Bolivia and Chile, and then Argentina and Chile. \\n    Socompa lies close to the pass of the same name where the Salta-Antofagasta railway crosses the Chilean border.\\n\\n    Most of the northwestern slope of Socompa collapsed catastrophically 7,200 years ago to form an extensive debris avalanche deposit. \\n    The Socompa collapse is among the largest known on land with a volume of 19.2 cubic kilometres (4.6 cu mi) and a surface area of 490 square \\n    kilometres (190 sq mi); its features are well-preserved by the arid climate. The deposit was at first considered to be either a moraine or a \\n    pyroclastic flow deposit, until the 1980 eruption of Mount St. Helens prompted awareness of the instability of volcanic edifices and the \\n    existence of large-scale collapses. There are large toreva blocks, which were left behind within the collapse crater. \\n    After the landslide, the volcano was rebuilt by the effusion of lava flows and much of the scar is now filled in.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/content.txt\", encoding='utf-8')\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9542904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 961.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\content.txt'}, page_content=' Socompa is a large stratovolcano (composite volcano) on the border of Argentina and Chile.\\n    It has an elevation of 6,051 metres (19,852 ft) and is part of the Chilean and Argentine Andean Volcanic Belt (AVB). \\n    Socompa is within the Central Volcanic Zone, one of the segments of the AVB, which contains about 44 active volcanoes. \\n    It begins in Peru and runs first through Bolivia and Chile, and then Argentina and Chile. \\n    Socompa lies close to the pass of the same name where the Salta-Antofagasta railway crosses the Chilean border.\\n\\n    Most of the northwestern slope of Socompa collapsed catastrophically 7,200 years ago to form an extensive debris avalanche deposit. \\n    The Socompa collapse is among the largest known on land with a volume of 19.2 cubic kilometres (4.6 cu mi) and a surface area of 490 square \\n    kilometres (190 sq mi); its features are well-preserved by the arid climate. The deposit was at first considered to be either a moraine or a \\n    pyroclastic flow deposit, until the 1980 eruption of Mount St. Helens prompted awareness of the instability of volcanic edifices and the \\n    existence of large-scale collapses. There are large toreva blocks, which were left behind within the collapse crater. \\n    After the landslide, the volcano was rebuilt by the effusion of lava flows and much of the scar is now filled in.\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Directory loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob = \"**/*.txt\",\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=True\n",
    ")\n",
    "dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd16c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AgenticAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls= PyMuPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "all_pdf_documents = dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370a30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7110d9a",
   "metadata": {},
   "source": [
    "## Embedding and Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306718c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbb17b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example chunk\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a9f75d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 11 documents into 43 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz...\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "708cca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadding Embedding Model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loadding Embedding Model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise \n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        print(f\"Generate embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generate embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Initialise the embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3751e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x18afbb577d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce7cc5",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2614b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectore store initialized. Collection: pdf_documents\n",
      "Existent document in collection: 0\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory:str = \"../data/vector_store\") :\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.initialize_store()\n",
    "\n",
    "    def initialize_store(self):\n",
    "        try:\n",
    "            # Create a persist chroma db client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True) \n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embedding for rag\"}\n",
    "            )\n",
    "            print(f\"Vectore store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existent document in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding: {len(documents)} documents to vector store\")\n",
    "\n",
    "        # Prepare data for chromedb\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #Prepare the metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document Content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids = ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "\n",
    "\n",
    "vector_store = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c4f51a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x18afbe434d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d319529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings for 43 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings with shape: (43, 384)\n",
      "Adding: 43 documents to vector store\n",
      "Successfully added 43 documents to vector store\n",
      "Total documents in collection: 43\n",
      "Collection count: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Convert the text to embeddings\n",
    "text = [doc.page_content for doc in chunks]\n",
    "\n",
    "\n",
    "# Generate the embeddings \n",
    "embeddings = embedding_manager.generate_embeddings(texts=text)\n",
    "\n",
    "# store in the vector database\n",
    "vector_store.add_documents(documents=chunks, embeddings=embeddings)\n",
    "print(\"Collection count:\", vector_store.collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdf813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08a2d85b",
   "metadata": {},
   "source": [
    "## Retriever pipeline from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d38bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager) :\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k:int = 5, score_threshold:float = -1) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"Top k: {top_k}, Score Threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings = [query_embedding.tolist()],\n",
    "                n_results = top_k\n",
    "            )\n",
    "\n",
    "            #Process Results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in  enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # convert distance to similarity score(chromadb uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(\"Error during query:\", e)\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vector_store, embedding_manager=embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f780a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: what is attention?\n",
      "Top k: 5, Score Threshold: -1\n",
      "Generate embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_c6fe3612_25',\n",
       "  'content': 'convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece',\n",
       "  'metadata': {'format': 'PDF 1.3',\n",
       "   'keywords': '',\n",
       "   'trapped': '',\n",
       "   'producer': 'PyPDF2',\n",
       "   'creationDate': '',\n",
       "   'creationdate': '',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'content_length': 969,\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'modDate': \"D:20180212212210-08'00'\",\n",
       "   'title': 'Attention is All you Need',\n",
       "   'total_pages': 11,\n",
       "   'page': 6,\n",
       "   'doc_index': 25,\n",
       "   'creator': '',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/'},\n",
       "  'similarity_score': -0.03572797775268555,\n",
       "  'distance': 1.0357279777526855,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_52496eee_11',\n",
       "  'content': 'around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3',\n",
       "  'metadata': {'total_pages': 11,\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'creationDate': '',\n",
       "   'format': 'PDF 1.3',\n",
       "   'creationdate': '',\n",
       "   'modDate': \"D:20180212212210-08'00'\",\n",
       "   'doc_index': 11,\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'trapped': '',\n",
       "   'page': 2,\n",
       "   'keywords': '',\n",
       "   'producer': 'PyPDF2',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'content_length': 967,\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'creator': ''},\n",
       "  'similarity_score': -0.07647085189819336,\n",
       "  'distance': 1.0764708518981934,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_22f2e34d_14',\n",
       "  'content': 'we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4',\n",
       "  'metadata': {'modDate': \"D:20180212212210-08'00'\",\n",
       "   'title': 'Attention is All you Need',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'trapped': '',\n",
       "   'creationdate': '',\n",
       "   'producer': 'PyPDF2',\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'format': 'PDF 1.3',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'keywords': '',\n",
       "   'total_pages': 11,\n",
       "   'creator': '',\n",
       "   'content_length': 835,\n",
       "   'page': 3,\n",
       "   'doc_index': 14,\n",
       "   'creationDate': ''},\n",
       "  'similarity_score': -0.08132028579711914,\n",
       "  'distance': 1.0813202857971191,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_e9d0688f_6',\n",
       "  'content': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions',\n",
       "  'metadata': {'trapped': '',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'creator': '',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'keywords': '',\n",
       "   'page': 1,\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'doc_index': 6,\n",
       "   'total_pages': 11,\n",
       "   'modDate': \"D:20180212212210-08'00'\",\n",
       "   'producer': 'PyPDF2',\n",
       "   'creationDate': '',\n",
       "   'format': 'PDF 1.3',\n",
       "   'content_length': 928,\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'creationdate': ''},\n",
       "  'similarity_score': -0.09461522102355957,\n",
       "  'distance': 1.0946152210235596,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_41c5d728_12',\n",
       "  'content': 'Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is',\n",
       "  'metadata': {'modDate': \"D:20180212212210-08'00'\",\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'doc_index': 12,\n",
       "   'content_length': 962,\n",
       "   'format': 'PDF 1.3',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'trapped': '',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'creationDate': '',\n",
       "   'creationdate': '',\n",
       "   'total_pages': 11,\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need-Paper.pdf',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'keywords': '',\n",
       "   'producer': 'PyPDF2',\n",
       "   'page': 3,\n",
       "   'creator': ''},\n",
       "  'similarity_score': -0.11243951320648193,\n",
       "  'distance': 1.112439513206482,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"what is attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb38c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa68b47a",
   "metadata": {},
   "source": [
    "## Integrate vectordb context pipeline with llm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3389f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple rag pipeline with groq llm\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "## initialize the groq llm\n",
    "groq_api_key1 = os.getenv(\"GROQ_API_KEY1\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key = groq_api_key1, model=\"gemma2-9b-it\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "##simple rag function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k = 3):\n",
    "    ## retrieve the context\n",
    "    results = retriever.retrieve(query, top_k = top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found\"\n",
    "    \n",
    "    # generate the answer with llm\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed4825ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: what is attention?\n",
      "Top k: 3, Score Threshold: -1\n",
      "Generate embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is a function that maps a query and a set of key-value pairs to an output, where the output is a weighted sum of the values, with the weights determined by the compatibility of the query with each key. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"what is attention?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb07664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ LLM working: Machine learning is the ability of computers to learn from data without being explicitly programmed....\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"What is machine learning? Answer in one sentence.\"\n",
    "response = llm.invoke(test_prompt)\n",
    "print(f\"   ✓ LLM working: {response.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8743a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351d5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03afbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6d223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bb39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
